---
permalink: /
title: "About"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I'm currently a [KTP Associate](https://www.ktp-uk.org/) (a special kind of postdoc) for both [University of Leeds](https://www.leeds.ac.uk/) and [TurinTech AI](https://www.turintech.ai/), and I'm working on cutting-edge compiler and LLM-based techniques for code optimization. 
At the University of Leeds, I am a member of the [<kbd>DSS Research Group</kbd>](https://distributed-systems.leeds.ac.uk/) and supervised by [Prof Jie Xu](https://eps.leeds.ac.uk/computing/staff/331/professor-jie-xu), and [Prof Zheng Wang](https://zwang4.github.io/). 
Moreover, at TurinTech AI, I'm a member of the Data Science team, which is led by [Dr Fan Wu](https://scholar.google.com/citations?user=p8z2_usAAAAJ&hl=en) and [Dr Paul Brookes](https://www.linkedin.com/in/paul-brookes-125288b2/).


I completed my PhD degree in Dec 2024 in [the Department of Computer Science](https://www.lboro.ac.uk/departments/compsci/) 
at [Loughborough University](https://www.lboro.ac.uk/). I'm very honored to be a member of the [<kbd>IDEAS Labotory</kbd>](https://ideas-labo.github.io/)
and be supervised by [Dr Tao Chen](https://scholar.google.co.uk/citations?user=K4teyvoAAAAJ&hl=en) during my doctoral study. 

My key research interests are in deep learning for software performance prediction. 
Iâ€™m passionate about this field because it has the potential to revolutionize the way we develop software. 

On this website, you can find more information about my research interests and projects. 
Feel free to contact me if you have any questions or would like to collaborate.

## News
>* **November/2024:** Our paper '[Accuracy Can Lie: On the Impact of Surrogate Model in Configuration Tuning](https://arxiv.org/abs/2409.07629)' has been accepted by the *IEEE Transactions on Software Engineering* [(TSE)](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=32) as a journal paper.
>
> 
>* **October/2024:** Our paper '[Dividable Configuration Performance Learning](https://arxiv.org/abs/2409.07629)' has been accepted by the *IEEE Transactions on Software Engineering* [(TSE)](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=32) as a journal paper.
>
> 
>* **August/2024:** Our paper '[Deep Configuration Performance Learning: A Systematic Survey and Taxonomy](https://arxiv.org/abs/2403.03322)' has been accepted by the *ACM Transactions on Software Engineering and Methodology* [(TOSEM)](https://dl.acm.org/journal/tosem) as a survey paper.
>
> 
>* **July/2024:** Our paper '[GreenStableYolo: Optimizing Inference Time and Image Quality of Text-to-Image Generation](https://arxiv.org/abs/2407.14982)' has been awarded the winner of the SSBSE'24 challenge track, thanks and congratulations to all the authors! 
>
> 
>* **May/2024:** Our paper '[GreenStableYolo: Optimizing Inference Time and Image Quality of Text-to-Image Generation](https://arxiv.org/abs/2407.14982)' has been accepted by the *Symposium on Search-Based Software Engineering* [(SSBSE 2024)](https://conf.researchr.org/track/ssbse-2024/ssbse-2024-challenge) as a challange track paper. 
>
> 
>* **January/2024:** Our paper '[Predicting Configuration Performance in Multiple Environments with Sequential Meta-Leaning](https://arxiv.org/abs/2402.03183)' has been accepted by the *ACM International Conference on the Foundations of Software Engineering* [(FSE 2024)](https://conf.researchr.org/home/fse-2024) as a research paper with acceptance rate 11.6% (56 out of 483). 
>
>
>* **May/2023:** Our paper '[Predicting Software Performance with Divide-and-Learn](https://arxiv.org/pdf/2306.06651)' has been accepted by the *ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering* [(ESEC/FSE 2023)](https://2023.esec-fse.org/) as a research paper with two strong accepts and no revision requested; acceptance rate 12.7% (60 out of 473). 
>
>
>* **May/2022:** Our paper '[Does Configuration Encoding Matter in Learning Software Performance? An Empirical Study on Encoding Schemes](https://arxiv.org/abs/2203.15988)' 
has been accepted by the *19th International Conference on Mining Software Repositories* ([MSR 2022](https://conf.researchr.org/details/msr-2022/msr-2022-technical-papers/1/Does-Configuration-Encoding-Matter-in-Learning-Software-Performance-An-Empirical-Stu)) 
as a technical paper, with an acceptance rate of 34% (45 out of 138).

## Publications
* <div style="text-align: justify"><span style="background-color:#5cb85c;display: inline;padding: .2em .6em .3em;font-size: 75%;font-weight: bold;line-height: 1;color: #ffffff;text-align: center;white-space: nowrap;vertical-align: baseline;border-radius: .25em;">TSE</span> <span style="background-color:#e06666;display: inline;padding: .2em .6em .3em;font-size: 75%;font-weight: bold;line-height: 1;color: #ffffff;text-align: center;white-space: nowrap;vertical-align: baseline;border-radius: .25em;">CCF-A</span> <span style="background-color:#0087BD;display: inline;padding: .2em .6em .3em;font-size: 75%;font-weight: bold;line-height: 1;color: #ffffff;text-align: center;white-space: nowrap;vertical-align: baseline;border-radius: .25em;">JCR-Q1</span> P. Chen, <b>J. Gong</b>, and T. Chen, <a href="https://arxiv.org/abs/2409.07629">Accuracy Can Lie: On the Impact of Surrogate Model in Configuration Tuning</a>, The IEEE Transactions on Software Engineering <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=32">(TSE)</a>, 2024, 33 pages. </div>

* <div style="text-align: justify"><span style="background-color:#5cb85c;display: inline;padding: .2em .6em .3em;font-size: 75%;font-weight: bold;line-height: 1;color: #ffffff;text-align: center;white-space: nowrap;vertical-align: baseline;border-radius: .25em;">TSE</span> <span style="background-color:#e06666;display: inline;padding: .2em .6em .3em;font-size: 75%;font-weight: bold;line-height: 1;color: #ffffff;text-align: center;white-space: nowrap;vertical-align: baseline;border-radius: .25em;">CCF-A</span> <span style="background-color:#0087BD;display: inline;padding: .2em .6em .3em;font-size: 75%;font-weight: bold;line-height: 1;color: #ffffff;text-align: center;white-space: nowrap;vertical-align: baseline;border-radius: .25em;">JCR-Q1</span> <b>J. Gong</b>, T. Chen, and R. Bahsoon, <a href="https://arxiv.org/abs/2409.07629">Dividable Configuration Performance Learningy</a>, The IEEE Transactions on Software Engineering <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=32">(TSE)</a>, 2024, 29 pages. </div>

* <div style="text-align: justify"><span style="background-color:#5cb85c;display: inline;padding: .2em .6em .3em;font-size: 75%;font-weight: bold;line-height: 1;color: #ffffff;text-align: center;white-space: nowrap;vertical-align: baseline;border-radius: .25em;">TOSEM</span> <span style="background-color:#e06666;display: inline;padding: .2em .6em .3em;font-size: 75%;font-weight: bold;line-height: 1;color: #ffffff;text-align: center;white-space: nowrap;vertical-align: baseline;border-radius: .25em;">CCF-A</span> <span style="background-color:#0087BD;display: inline;padding: .2em .6em .3em;font-size: 75%;font-weight: bold;line-height: 1;color: #ffffff;text-align: center;white-space: nowrap;vertical-align: baseline;border-radius: .25em;">JCR-Q1</span> <b>J. Gong</b> and T. Chen, <a href="https://arxiv.org/abs/2403.03322">Deep Configuration Performance Learning: A Systematic Survey and Taxonomy</a>, The ACM Transactions on Software Engineering and Methodology <a href="https://dl.acm.org/journal/tosem">(TOSEM)</a>, 2024, 62 pages. </div>

* <div style="text-align: justify"><span style="background-color:#5cb85c;display: inline;padding: .2em .6em .3em;font-size: 75%;font-weight: bold;line-height: 1;color: #ffffff;text-align: center;white-space: nowrap;vertical-align: baseline;border-radius: .25em;">SSBSE 2024</span> <span style="background-color:#e06666;display: inline;padding: .2em .6em .3em;font-size: 75%;font-weight: bold;line-height: 1;color: #ffffff;text-align: center;white-space: nowrap;vertical-align: baseline;border-radius: .25em;">Challenge Winner</span> <span style="background-color:#0087BD;display: inline;padding: .2em .6em .3em;font-size: 75%;font-weight: bold;line-height: 1;color: #ffffff;text-align: center;white-space: nowrap;vertical-align: baseline;border-radius: .25em;">CORE-B</span> <b>J. Gong</b>, S Li, G d'Aloisio, Z Ding, Y Ye, W Langdon and F Sarro, <a href="https://arxiv.org/abs/2407.14982">GreenStableYolo: Optimizing Inference Time and Image Quality of Text-to-Image Generation</a>, The Symposium on Search-Based Software Engineering Challenge Track <a href="https://conf.researchr.org/track/ssbse-2024/ssbse-2024-challenge">(SSBSE 2024)</a>, 6 pages. </div>

* <div style="text-align: justify"><span style="background-color:#5cb85c;display: inline;padding: .2em .6em .3em;font-size: 75%;font-weight: bold;line-height: 1;color: #ffffff;text-align: center;white-space: nowrap;vertical-align: baseline;border-radius: .25em;">FSE 2024</span> <span style="background-color:#e06666;display: inline;padding: .2em .6em .3em;font-size: 75%;font-weight: bold;line-height: 1;color: #ffffff;text-align: center;white-space: nowrap;vertical-align: baseline;border-radius: .25em;">CCF-A</span> <span style="background-color:#0087BD;display: inline;padding: .2em .6em .3em;font-size: 75%;font-weight: bold;line-height: 1;color: #ffffff;text-align: center;white-space: nowrap;vertical-align: baseline;border-radius: .25em;">CORE-A*</span> <b>J. Gong</b> and T. Chen, <a href="https://arxiv.org/abs/2402.03183">Predicting Configuration Performance in Multiple Environments with Sequential Meta-Leaning</a>, The ACM International Conference on the Foundations of Software Engineering <a href="https://conf.researchr.org/home/fse-2024">(FSE 2024)</a>, 24 pages. </div>

* <div style="text-align: justify"><span style="background-color:#5cb85c;display: inline;padding: .2em .6em .3em;font-size: 75%;font-weight: bold;line-height: 1;color: #ffffff;text-align: center;white-space: nowrap;vertical-align: baseline;border-radius: .25em;">ESEC/FSE 2023</span> <span style="background-color:#e06666;display: inline;padding: .2em .6em .3em;font-size: 75%;font-weight: bold;line-height: 1;color: #ffffff;text-align: center;white-space: nowrap;vertical-align: baseline;border-radius: .25em;">CCF-A</span> <span style="background-color:#0087BD;display: inline;padding: .2em .6em .3em;font-size: 75%;font-weight: bold;line-height: 1;color: #ffffff;text-align: center;white-space: nowrap;vertical-align: baseline;border-radius: .25em;">CORE-A*</span> <b>J. Gong</b> and T. Chen, <a href="https://arxiv.org/pdf/2306.06651">Predicting Software Performance with Divide-and-Learn</a>, The ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering <a href="https://2023.esec-fse.org/">(ESEC/FSE 2023)</a>, 13 pages. </div>

* <div style="text-align: justify"><span style="background-color:#5cb85c;display: inline;padding: .2em .6em .3em;font-size: 75%;font-weight: bold;line-height: 1;color: #ffffff;text-align: center;white-space: nowrap;vertical-align: baseline;border-radius: .25em;">MSR 2022</span> <span style="background-color:#e06666;display: inline;padding: .2em .6em .3em;font-size: 75%;font-weight: bold;line-height: 1;color: #ffffff;text-align: center;white-space: nowrap;vertical-align: baseline;border-radius: .25em;">CCF-C</span> <span style="background-color:#0087BD;display: inline;padding: .2em .6em .3em;font-size: 75%;font-weight: bold;line-height: 1;color: #ffffff;text-align: center;white-space: nowrap;vertical-align: baseline;border-radius: .25em;">CORE-A</span> <b>J. Gong</b> and T. Chen, <a href="https://arxiv.org/abs/2203.15988">Does Configuration Encoding Matter in Learning Software Performance? An Empirical Study on Encoding Schemes</a>, The International Conference on Mining Software Repositories <a href="https://conf.researchr.org/details/msr-2022/msr-2022-technical-papers/1/Does-Configuration-Encoding-Matter-in-Learning-Software-Performance-An-Empirical-Stu">(MSR 2022)</a>, 13 pages. </div>


## Research Interests
My doctoral research interests are in applying **machine learning (deep learning)** to learn **software configurations and performance**. 
This involves modeling the complex relationship between the configurable options of the software and its performance metrics (such as latency, execution time, etc.) 
and accurately predicting the performance for any configuration. 

This way, software engineers and users can optimize the software to meet their design/execution requirements without spending much time testing the software. 
Moreover, machine learning (deep learning) models can reduce the large amount of costs on performance measurements due to their high efficiency of learning.

In my postdoc research, I'm focusing on applying large language models for software code optimization.

## Further Backgrounds
I received first-class BSc degree from both the [Information and Computing Science](https://www.xjtlu.edu.cn/en/study/undergraduate/information-and-computing-science)
programme at [Xi'an Jiaotong-Liverpool University](https://www.xjtlu.edu.cn/) (2014-16), and the [Computer Science](https://www.liverpool.ac.uk/courses/2024/computer-science-bsc-hons) 
course at [University of Liverpool](https://www.liverpool.ac.uk/) (2016-18). 